{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import VADER sentiment analyzer\n",
    "import pandas as pd\n",
    "import nltk.sentiment\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Download required lexicon if not already present\n",
    "try:\n",
    "    nltk.download('vader_lexicon', quiet=True)\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading vader_lexicon: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from the extracted path instead of zip file\n",
    "df= pd.read_csv(r\"C:\\Users\\LENOVO\\Practice_Twitter.zip\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Data Preprocessing and Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    2.1 VADER Sentiment Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Condition\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Function to get sentiment label\n",
    "def get_sentiment(text):\n",
    "    # Get the polarity scores using VADER\n",
    "    scores = sid.polarity_scores(text)\n",
    "    # Determine sentiment based on the compound score\n",
    "    if scores['compound'] >= 0.05:\n",
    "        return 'Positive'\n",
    "    elif scores['compound'] <= -0.05:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File successfully saved to: C:\\Users\\LENOVO\\Practice_Twittersent.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "    # Apply sentiment analysis to the 'Heading' column\n",
    "    df['Sentiment'] = df['Heading'].apply(get_sentiment)\n",
    "    \n",
    "    # Define output path using os.path for better cross-platform compatibility\n",
    "    Twitter_sentiment = os.path.join(os.path.expanduser(\"~\"), \"Practice_Twittersent.csv\")\n",
    "    \n",
    "    # Save to CSV with error handling\n",
    "    df.to_csv(Twitter_sentiment, index=False)\n",
    "    print(f\"File successfully saved to: {Twitter_sentiment}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14000 entries, 0 to 13999\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   Heading    14000 non-null  object\n",
      " 1   Sentiment  14000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 218.9+ KB\n",
      "None\n",
      "\n",
      "First few rows:\n",
      "                                             Heading Sentiment\n",
      "0  is upset that he can't update his Facebook by ...  Negative\n",
      "1  @Kenichan I dived many times for the ball. Man...  Positive\n",
      "2    my whole body feels itchy and like its on fire   Negative\n",
      "3  @nationwideclass no, it's not behaving at all....  Negative\n",
      "4                      @Kwesidei not the whole crew    Neutral\n",
      "\n",
      "Sentiment Distribution:\n",
      "Sentiment\n",
      "Negative    5776\n",
      "Positive    4335\n",
      "Neutral     3889\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Read the CSV file using the same path where we saved it earlier\n",
    "    tweet = pd.read_csv(Twitter_sentiment)  # Using the path variable we defined earlier\n",
    "    \n",
    "    # Print basic information about the dataset\n",
    "    print(\"\\nDataset Info:\")\n",
    "    print(tweet.info())\n",
    "    \n",
    "    print(\"\\nFirst few rows:\")\n",
    "    print(tweet.head())\n",
    "    \n",
    "    print(\"\\nSentiment Distribution:\")\n",
    "    print(tweet['Sentiment'].value_counts())\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found at {Twitter_sentiment}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while reading the file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label Distribution:\n",
      "0    5776\n",
      "2    4335\n",
      "1    3889\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Mapping:\n",
      "2 = Positive\n",
      "1 = Neutral\n",
      "0 = Negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_8552\\2701927848.py:17: FutureWarning: pandas.value_counts is deprecated and will be removed in a future version. Use pd.Series(obj).value_counts() instead.\n",
      "  print(pd.value_counts(labels))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create numerical labels for all sentiment classes\n",
    "labels = np.where(tweet['Sentiment'] == 'Positive', 2,\n",
    "                 np.where(tweet['Sentiment'] == 'Neutral', 1, 0))\n",
    "\n",
    "# Alternative approach using label encoding\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# le = LabelEncoder()\n",
    "# labels = le.fit_transform(tweet['Sentiment'])\n",
    "\n",
    "# Add the numerical labels back to the dataframe if needed\n",
    "tweet['sentiment_label'] = labels\n",
    "\n",
    "# Verify the encoding\n",
    "print(\"\\nLabel Distribution:\")\n",
    "print(pd.value_counts(labels))\n",
    "print(\"\\nMapping:\")\n",
    "print(\"2 = Positive\")\n",
    "print(\"1 = Neutral\")\n",
    "print(\"0 = Negative\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    2.2 Data Cleaning and Text Processing\n",
    "        The following steps are performed:\n",
    "        - Remove URLs, hashtags, and mentions\n",
    "        - Convert text to lowercase\n",
    "        - Remove punctuation and special characters\n",
    "        - Tokenization\n",
    "        - Remove stopwords\n",
    "        - Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading stopwords: {e}\")\n",
    "\n",
    "def process_tweet(tweet):\n",
    "    \"\"\"Process tweet function.\n",
    "    Input:\n",
    "        tweet: a string containing a tweet\n",
    "    Output:\n",
    "        tweets_clean: a list of words containing the processed tweet\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Handle None or non-string inputs\n",
    "        if tweet is None or not isinstance(tweet, str):\n",
    "            return []\n",
    "            \n",
    "        stemmer = PorterStemmer()\n",
    "        stopwords_english = stopwords.words('english')\n",
    "        \n",
    "        # Convert to lowercase\n",
    "        tweet = tweet.lower()\n",
    "        \n",
    "        # Remove stock market tickers like $GE\n",
    "        tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "        # Remove old style retweet text \"RT\"\n",
    "        tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "        # Remove hyperlinks\n",
    "        tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "        # Remove hashtags (only the # symbol)\n",
    "        tweet = re.sub(r'#', '', tweet)\n",
    "        # Remove emojis and special characters\n",
    "        tweet = re.sub(r'[^\\x00-\\x7F]+', '', tweet)\n",
    "        \n",
    "        # Tokenize tweets\n",
    "        tokenizer = TweetTokenizer(preserve_case=False, \n",
    "                                 strip_handles=True,\n",
    "                                 reduce_len=True)\n",
    "        tweet_tokens = tokenizer.tokenize(tweet)\n",
    "\n",
    "        tweets_clean = []\n",
    "        for word in tweet_tokens:\n",
    "            if (word not in stopwords_english and  # remove stopwords\n",
    "                word not in string.punctuation):   # remove punctuation\n",
    "                stem_word = stemmer.stem(word)     # stemming word\n",
    "                tweets_clean.append(stem_word)\n",
    "\n",
    "        return tweets_clean\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing tweet: {e}\")\n",
    "        return []\n",
    "\n",
    "def build_freqs(tweets, ys):\n",
    "    \"\"\"Build frequencies.\n",
    "    Input:\n",
    "        tweets: a list of tweets\n",
    "        ys: an m x 1 array with the sentiment labels\n",
    "    Output:\n",
    "        freqs: a dictionary mapping each (word, sentiment) pair to its frequency\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convert np array to list\n",
    "        yslist = np.squeeze(ys).tolist()\n",
    "\n",
    "        if len(tweets) != len(yslist):\n",
    "            raise ValueError(\"Number of tweets and labels must match\")\n",
    "\n",
    "        # Build frequency dictionary\n",
    "        freqs = {}\n",
    "        for y, tweet in zip(yslist, tweets):\n",
    "            for word in tweet:\n",
    "                pair = (word, y)\n",
    "                freqs[pair] = freqs.get(pair, 0) + 1\n",
    "                \n",
    "        return freqs\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error building frequencies: {e}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing tweets...\n",
      "\n",
      "Processed 14000 tweets\n",
      "\n",
      "Sample of processed tweets:\n",
      "Tweet 1: ['upset', \"can't\", 'updat', 'facebook', 'text', '...', 'might', 'cri', 'result', 'school', 'today', 'also', 'blah']\n",
      "Tweet 2: ['dive', 'mani', 'time', 'ball', 'manag', 'save', '50', 'rest', 'go', 'bound']\n",
      "Tweet 3: ['whole', 'bodi', 'feel', 'itchi', 'like', 'fire']\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Process tweets with progress tracking\n",
    "    print(\"Processing tweets...\")\n",
    "    tweet['Heading'] = tweet['Heading'].apply(process_tweet)\n",
    "    \n",
    "    # Verify processing\n",
    "    print(f\"\\nProcessed {len(tweet)} tweets\")\n",
    "    \n",
    "    # Show a sample of processed tweets\n",
    "    print(\"\\nSample of processed tweets:\")\n",
    "    for i, processed_tweet in enumerate(tweet['Heading'].head(3)):\n",
    "        print(f\"Tweet {i+1}: {processed_tweet}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error processing tweets: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 11200\n",
      "Test set size: 2800\n",
      "\n",
      "Label distribution in training set:\n",
      "sentiment_label\n",
      "0    4621\n",
      "2    3468\n",
      "1    3111\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Label distribution in test set:\n",
      "sentiment_label\n",
      "0    1155\n",
      "2     867\n",
      "1     778\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "try:\n",
    "    # Use the sentiment_label column we created earlier instead of 'type'\n",
    "    X = tweet.Heading\n",
    "    y = tweet.sentiment_label  # or tweet.Sentiment for categorical labels\n",
    "    \n",
    "    # Create train-test split with stratification\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, \n",
    "        y, \n",
    "        test_size=0.2, \n",
    "        random_state=42,\n",
    "        stratify=y  # Ensure balanced classes in both splits\n",
    "    )\n",
    "    \n",
    "    # Print split information\n",
    "    print(f\"Training set size: {len(X_train)}\")\n",
    "    print(f\"Test set size: {len(X_test)}\")\n",
    "    print(\"\\nLabel distribution in training set:\")\n",
    "    print(pd.Series(y_train).value_counts())\n",
    "    print(\"\\nLabel distribution in test set:\")\n",
    "    print(pd.Series(y_test).value_counts())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during train-test split: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training data shape: (11200, 4649)\n",
      "Test data shape: (2800, 4649)\n",
      "Number of features: 4649\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "try:\n",
    "    # Convert tokens to strings for vectorization\n",
    "    X_train_strings = [' '.join(map(str, tokens)) for tokens in X_train]\n",
    "    X_test_strings = [' '.join(map(str, tokens)) for tokens in X_test]\n",
    "    \n",
    "    # Initialize vectorizer with parameters\n",
    "    vectorizer = CountVectorizer(\n",
    "        max_features=5000,  # Limit features to prevent memory issues\n",
    "        min_df=2,          # Ignore terms that appear in less than 2 documents\n",
    "        max_df=0.95        # Ignore terms that appear in more than 95% of documents\n",
    "    )\n",
    "    \n",
    "    # Fit and transform training data\n",
    "    X_train_cv = vectorizer.fit_transform(X_train_strings)\n",
    "    # Transform test data\n",
    "    X_test_cv = vectorizer.transform(X_test_strings)\n",
    "    \n",
    "    # Print vectorization results\n",
    "    print(f\"\\nTraining data shape: {X_train_cv.shape}\")\n",
    "    print(f\"Test data shape: {X_test_cv.shape}\")\n",
    "    print(f\"Number of features: {len(vectorizer.get_feature_names_out())}\")\n",
    "    \n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during vectorization: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Accuracy: 80.79%\n",
      "Test Accuracy: 66.96%\n",
      "\n",
      "Classification Report (Test Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.65      0.75      0.70      1155\n",
      "     Neutral       0.69      0.50      0.58       778\n",
      "    Positive       0.69      0.71      0.70       867\n",
      "\n",
      "    accuracy                           0.67      2800\n",
      "   macro avg       0.68      0.65      0.66      2800\n",
      "weighted avg       0.67      0.67      0.67      2800\n",
      "\n",
      "\n",
      "Confusion Matrix (Test Set):\n",
      "[[871 111 173]\n",
      " [285 388 105]\n",
      " [189  62 616]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    # Initialize and train the model\n",
    "    model = MultinomialNB()\n",
    "    model.fit(X_train_cv, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_train = model.predict(X_train_cv)\n",
    "    y_pred_test = model.predict(X_test_cv)\n",
    "    \n",
    "    # Print training scores\n",
    "    print(\"\\nTraining Accuracy: {:.2f}%\".format(model.score(X_train_cv, y_train) * 100))\n",
    "    print(\"Test Accuracy: {:.2f}%\".format(model.score(X_test_cv, y_test) * 100))\n",
    "    \n",
    "    # Print detailed classification report\n",
    "    print(\"\\nClassification Report (Test Set):\")\n",
    "    print(classification_report(y_test, y_pred_test, \n",
    "                              target_names=['Negative', 'Neutral', 'Positive']))\n",
    "    \n",
    "    # Print confusion matrix\n",
    "    print(\"\\nConfusion Matrix (Test Set):\")\n",
    "    print(confusion_matrix(y_test, y_pred_test))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during model training and evaluation: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Accuracy: 50.96%\n",
      "Test Accuracy: 50.07%\n",
      "\n",
      "Classification Report (Test Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.45      0.92      0.61      1155\n",
      "     Neutral       0.00      0.00      0.00       778\n",
      "    Positive       0.76      0.39      0.51       867\n",
      "\n",
      "    accuracy                           0.50      2800\n",
      "   macro avg       0.40      0.44      0.37      2800\n",
      "weighted avg       0.42      0.50      0.41      2800\n",
      "\n",
      "\n",
      "Confusion Matrix (Test Set):\n",
      "[[1065    0   90]\n",
      " [ 763    0   15]\n",
      " [ 529    1  337]]\n",
      "\n",
      "Top 10 Most Important Features:\n",
      "wish: 0.1416\n",
      "hope: 0.1152\n",
      "love: 0.0976\n",
      "lol: 0.0836\n",
      "hate: 0.0783\n",
      "sad: 0.0733\n",
      "good: 0.0721\n",
      "like: 0.0711\n",
      "miss: 0.0706\n",
      "bad: 0.0688\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    # Initialize Decision Tree with parameters\n",
    "    model = tree.DecisionTreeClassifier(\n",
    "        max_depth=10,           # Prevent overfitting\n",
    "        min_samples_split=5,    # Minimum samples required to split\n",
    "        min_samples_leaf=2,     # Minimum samples required at leaf node\n",
    "        random_state=42         # For reproducibility\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train_cv, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_train = model.predict(X_train_cv)\n",
    "    y_pred_test = model.predict(X_test_cv)\n",
    "    \n",
    "    # Print scores\n",
    "    print(\"\\nTraining Accuracy: {:.2f}%\".format(model.score(X_train_cv, y_train) * 100))\n",
    "    print(\"Test Accuracy: {:.2f}%\".format(model.score(X_test_cv, y_test) * 100))\n",
    "    \n",
    "    # Print detailed classification report\n",
    "    print(\"\\nClassification Report (Test Set):\")\n",
    "    print(classification_report(y_test, y_pred_test,\n",
    "                              target_names=['Negative', 'Neutral', 'Positive']))\n",
    "    \n",
    "    # Print confusion matrix\n",
    "    print(\"\\nConfusion Matrix (Test Set):\")\n",
    "    print(confusion_matrix(y_test, y_pred_test))\n",
    "    \n",
    "    # Optional: Print feature importance\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    importances = model.feature_importances_\n",
    "    indices = np.argsort(importances)[-10:]  # Get indices of top 10 features\n",
    "    \n",
    "    print(\"\\nTop 10 Most Important Features:\")\n",
    "    for idx in reversed(indices):\n",
    "        print(f\"{feature_names[idx]}: {importances[idx]:.4f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    \n",
    "    print(f\"Error during model training and evaluation: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 54.67%\n",
      "Test Accuracy: 54.14%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.47      0.95      0.63      1155\n",
      "     Neutral       0.00      0.00      0.00       778\n",
      "    Positive       0.85      0.49      0.62       867\n",
      "\n",
      "    accuracy                           0.54      2800\n",
      "   macro avg       0.44      0.48      0.42      2800\n",
      "weighted avg       0.46      0.54      0.45      2800\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1092    0   63]\n",
      " [ 765    0   13]\n",
      " [ 443    0  424]]\n",
      "\n",
      "Top 10 Most Important Features:\n",
      "wish: 0.0769\n",
      "love: 0.0450\n",
      "hope: 0.0429\n",
      "good: 0.0366\n",
      "hate: 0.0363\n",
      "sick: 0.0356\n",
      "lol: 0.0348\n",
      "bad: 0.0266\n",
      "like: 0.0259\n",
      "thank: 0.0221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\LENOVO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\LENOVO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "try:\n",
    "    # Initialize Random Forest with optimized parameters\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=100,      # More trees for better performance\n",
    "        max_depth=20,          # Control tree depth to prevent overfitting\n",
    "        min_samples_split=5,   # Minimum samples required to split\n",
    "        min_samples_leaf=2,    # Minimum samples at leaf nodes\n",
    "        random_state=42,       # For reproducibility\n",
    "        n_jobs=-1              # Use all available cores\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train_cv, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_train = model.predict(X_train_cv)\n",
    "    y_pred_test = model.predict(X_test_cv)\n",
    "    \n",
    "    # Print accuracies\n",
    "    print(f\"Training Accuracy: {accuracy_score(y_train, y_pred_train):.2%}\")\n",
    "    print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_test):.2%}\")\n",
    "    \n",
    "    # Print detailed classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred_test,\n",
    "                              target_names=['Negative', 'Neutral', 'Positive']))\n",
    "    \n",
    "    # Print confusion matrix\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred_test))\n",
    "    \n",
    "    # Print feature importance\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    importances = model.feature_importances_\n",
    "    indices = np.argsort(importances)[-10:]  # Get top 10 features\n",
    "    \n",
    "    print(\"\\nTop 10 Most Important Features:\")\n",
    "    for idx in reversed(indices):\n",
    "        print(f\"{feature_names[idx]}: {importances[idx]:.4f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during model training and evaluation: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVM model (this may take a while)...\n",
      "\n",
      "Training Accuracy: 90.38%\n",
      "Test Accuracy: 75.68%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.79      0.73      0.76      1155\n",
      "     Neutral       0.73      0.82      0.77       778\n",
      "    Positive       0.75      0.74      0.74       867\n",
      "\n",
      "    accuracy                           0.76      2800\n",
      "   macro avg       0.75      0.76      0.76      2800\n",
      "weighted avg       0.76      0.76      0.76      2800\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[840 151 164]\n",
      " [ 87 638  53]\n",
      " [142  84 641]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "try:\n",
    "    # Initialize SVM with optimized parameters\n",
    "    model = SVC(\n",
    "        kernel='linear',     # Linear kernel for text classification\n",
    "        C=1.0,              # Regularization parameter\n",
    "        random_state=42,     # For reproducibility\n",
    "        probability=True,    # Enable probability estimates\n",
    "        class_weight='balanced'  # Handle class imbalance\n",
    "    )\n",
    "    \n",
    "    print(\"Training SVM model (this may take a while)...\")\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train_cv, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_train = model.predict(X_train_cv)\n",
    "    y_pred_test = model.predict(X_test_cv)\n",
    "    \n",
    "    # Print accuracies\n",
    "    print(f\"\\nTraining Accuracy: {accuracy_score(y_train, y_pred_train):.2%}\")\n",
    "    print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_test):.2%}\")\n",
    "    \n",
    "    # Print detailed classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred_test,\n",
    "                              target_names=['Negative', 'Neutral', 'Positive']))\n",
    "    \n",
    "    # Print confusion matrix\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred_test))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during model training and evaluation: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Prediction Function\n",
    "Test the model with custom input text to predict sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input text: 'Murder'\n",
      "Predicted sentiment: Negative\n",
      "Confidence: 58.37%\n",
      "\n",
      "Class probabilities:\n",
      "Negative: 0.58\n",
      "Neutral: 0.25\n",
      "Positive: 0.16\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Input text to analyze\n",
    "    test_text = ['Murder']\n",
    "    \n",
    "    # Transform input using the same vectorizer\n",
    "    test_vector = vectorizer.transform(test_text)\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = model.predict(test_vector)\n",
    "    \n",
    "    # Get probability scores if model supports it\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        probabilities = model.predict_proba(test_vector)[0]\n",
    "        confidence = max(probabilities) * 100\n",
    "    else:\n",
    "        confidence = None\n",
    "    \n",
    "    # Map prediction to sentiment\n",
    "    sentiment_map = {\n",
    "        0: 'Negative',\n",
    "        1: 'Neutral',\n",
    "        2: 'Positive'\n",
    "    }\n",
    "    predicted_sentiment = sentiment_map.get(prediction[0], 'Unknown')\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nInput text: '{test_text[0]}'\")\n",
    "    print(f\"Predicted sentiment: {predicted_sentiment}\")\n",
    "    if confidence:\n",
    "        print(f\"Confidence: {confidence:.2f}%\")\n",
    "        \n",
    "    # Optional: Show probabilities for each class if available\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        print(\"\\nClass probabilities:\")\n",
    "        for sentiment, prob in zip(['Negative', 'Neutral', 'Positive'], probabilities):\n",
    "            print(f\"{sentiment}: {prob:.2f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during prediction: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
